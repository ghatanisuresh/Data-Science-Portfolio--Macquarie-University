# Reflective Report

## Suresh Ghatani 
### 46864512

<p style='text-align: justify;'>For the past 3 months, I have been learning numerous modelling techniques for the analysis of data and their implementation inducing the conclusion from the past datasets. During this time frame, I attained different machine learning processes and their application in the real world problems with applied descriptive statistics knowledge. Some of the techniques that I learned are as below, which are used for building models. Similarly, I understood that below mentioned methods are examples of Machine Learning to some degree.

   * Linear Regression
   * Logistic Regression
   * Clustering and Classification


The detailed process of solving the problem is, for sure,  by building the machine model as mentioned above. However, all these modelling techniques have the same process which  includes:
   * Loading the dataset.
   * Summarizing the dataset.
   * Dimensions of dataset
   * Statistical Summary
   * Class Distribution
   * Visualizing the dataset:
        - Univariate Plot
        - Multivariate Plot
   * Evaluating some algorithms.
        - Create a Validation Dataset
        - Split dataset into Training and Testing dataset with different size
   * Build Models: 
        - Simple Linear Regression
        - Logistic Regression
        - Decision Tree Classification
        - K-Nearest Neighbors (KNN)
        - Gaussian Naive Bayes
   * Making some predictions.
        - Making prediction
        - Evaluating Prediction


<p style='text-align: justify;'>In order to perform all this model’s operation, I gained knowledge about the IDE, the Jupyter notebook platform, which  is a more viable platform that provides the flexible interface allowing users to configure and arrange workflow in data science and machine learning. Similarly, I learned that it allows us to build a single document that includes both the data and analysis commentary as well as the code that generates the findings. As a result, my work became more productive. In the same way the ‘markdown feature’ of this notebook allows me to add both the plain text, notations, and code in one document, making it easier to manage workflows in a document. The features for the notebook markdown are: 


   * Headings
   * Blockquotes
   * Code section
   * Mathematical Symbol
   * Line Break
   * Bold and Italic Text
   * Horizontal Lines
   * Ordered List
   * Unordered List

 

<br>Moreover, graphical representation and interpretation of data in a notebook made me feel that data reasoning in a notebook is by far the most powerful tool to do so. Github, in the same way, is also one of my learned subjects that showed me the way to make repositories to store all of my work in a separate repository, and to track changes. Regarding the datasets, Lots of file formats such as word, pdf, XML, JSON, Excel are usually used, but I got more familiar with the CSV file format only.

<p style='text-align: justify;'>From the very beginning of this unit, I commenced through the  basic introduction of the data science terminology, and its related detailed lessons, gradually to different machine learning models techniques done via Python Programming Language. However, for the first phase , I was just limited to theoretical knowledge only. Slowly but surely,  I walked through the weekly practical workshops, and four- portfolios, due to which it made me capable of understanding to get the real insights of those theoretical ideas into practical implementation, and made me able to solve real world problems. Similarly, I got familiar with the different required python libraries such as scipy, numpy, matplotlib, pandas, sklearn, and seaborn. By the time I reached portfolio 4, I came to the detailed processes and steps for building the different supervised and unsupervised machine learning models for prediction, classification and clustering with graphical visualization, evaluation of the different models along with the analysis of the data. 
<br><br>As this unit, now, has made me somewhat capable of doing the general analysis and evaluation of the data and predicting the future outcomes from the previous datasets, I decided to implement the same algorithm in my father’s business in the coming future. My father runs a construction supplying business in Nepal. Therefore, I believe that with the aid of this learned model from this unit I can really predict and visualize graphically of my father’s business revenue, customer flow, likely and unlikely to buy from the potential customer, identifying the customer, and many more. Consequently, it will help businesses for marketing, and improving financial management. 


<p style='text-align: justify;'>Regarding portfolio 4, I was provided with the full access to choose from my own datasets, and identify the problem along with the purposes to the analysis. For this, I searched different data sources sites: kaggle, UCI Machine Learning, Data.Gov, and Australian Tax Office for the proper datasets. Finally, I chose the stroke prediction dataset “stroke.csv” from Kaggle because I found this data appropriate, especially for the prediction of how a patient is likely to get stroke, and classify what is the average age who are likely to get affected by stroke based on the different dependent variables: gender, age, various diseases, and smoking status for my proposed analysis, and I believed this dataset would be the best dataset for my analysis to build all the machine learning models and compare the models. 


<p style='text-align: justify;'>I implemented a different model for the dataset “stroke.csv” I chose. I applied a regression model: Simple and Logistic Regression model for the prediction of what factors cause the stroke the most. Choosing the regression model helps to identify the relation between the dependent variable and independent variable, which is the causing factor. As per my dataset, there were independent variables: age, hypertension, average glucose level, bmi, smoking status. Finding the correlation between these variables with stroke variables, age was found to have a strong relationship with the stroke, followed  by hypertension. Similarly, I performed a classification problem like the K-Nearest Neighbor and Naive Bayes classifier, where the stroke variable consists of binary numerical data, 1 if the patient had a stroke or 0 if not.    

<p style='text-align: justify;'>I firstly build a baseline model and following the advanced model on this dataset, and after evaluating the model with different evaluating criteria such as Mean Square Error, R Squared error for the regression problem, where as F-score and confusion matrix for the classification problem, and similarly, performed parameter tuning with cross validation. After measuring the accuracy of all the models K-Nearest deemed to be the best model and fitted well for my prediction analysis as compared to other models.


<p style='text-align: justify;'>After performing all these operations on the dataset I found that age is the highest significant factor, and is more correlated with stroke. The conclusion I drew from this analysis is “Chances of suffering from a stroke increases, as we get older and older”, and I would say whatever the insights that I had guessed before applying the model is actually as per my  anticipation. 


<p style='text-align: justify;'>What I would like to add more is that it was challenging with finding the proper dataset for the first and took me time to find out the exact dataset. Though after a great deal of research in various given data source sites, I was able to find a relevant dataset, and perform machine learning models to find the pattern on stroking, prediction of how likely to get the stroke after finding the relationship with the independent variables. Finally, used visualize using graphing technique to get the meaningful insights. 


```python

```
